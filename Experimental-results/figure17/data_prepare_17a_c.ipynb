{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6532d0-4b19-4361-adca-1170ca94873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Set\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "\n",
    "# Message size constants definition\n",
    "BASE_MESSAGE_SIZE = 40    # Base message header size (bytes)\n",
    "ISL_DELETE_SIZE = 53     # ISL deletion operation message size\n",
    "ISL_ADD_SIZE = 125      # ISL addition operation message size\n",
    "ROUTE_ENTRY_SIZE = 97   # Route table entry operation message size\n",
    "\n",
    "class NodeStats:\n",
    "    def __init__(self):\n",
    "        self.deleted_isls = 0\n",
    "        self.added_isls = 0\n",
    "        self.deleted_routes = 0\n",
    "        self.added_routes = 0\n",
    "        # Aggregated routing statistics\n",
    "        self.deleted_aggregated_routes = 0\n",
    "        self.added_aggregated_routes = 0\n",
    "        \n",
    "    def get_total_bytes(self):\n",
    "        return (BASE_MESSAGE_SIZE +\n",
    "                ISL_DELETE_SIZE * self.deleted_isls +\n",
    "                ISL_ADD_SIZE * self.added_isls +\n",
    "                ROUTE_ENTRY_SIZE * (self.deleted_routes + self.added_routes))\n",
    "                \n",
    "    def get_total_aggregated_bytes(self):\n",
    "        return (BASE_MESSAGE_SIZE +\n",
    "                ISL_DELETE_SIZE * self.deleted_isls +\n",
    "                ISL_ADD_SIZE * self.added_isls +\n",
    "                ROUTE_ENTRY_SIZE * (self.deleted_aggregated_routes + \n",
    "                                  self.added_aggregated_routes))\n",
    "\n",
    "def build_topology_graph(inter_topology, intra_topology):\n",
    "    \"\"\"Build a complete graph containing both intra-domain and inter-domain connections\"\"\"\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add inter-domain connections\n",
    "    for (node1, node2), _ in inter_topology:\n",
    "        G.add_edge(node1, node2)\n",
    "    \n",
    "    # Add intra-domain connections\n",
    "    for grid_connections in intra_topology.values():\n",
    "        for node1, node2 in grid_connections:\n",
    "            G.add_edge(node1, node2)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def get_topology_satellites(inter_topology, intra_topology):\n",
    "    \"\"\"Get all satellite nodes in the topology\"\"\"\n",
    "    satellites = set()\n",
    "    \n",
    "    # Get satellites from inter-domain topology\n",
    "    for (node1, node2), _ in inter_topology:\n",
    "        satellites.add(node1)\n",
    "        satellites.add(node2)\n",
    "    \n",
    "    # Get satellites from intra-domain topology\n",
    "    for grid_connections in intra_topology.values():\n",
    "        for node1, node2 in grid_connections:\n",
    "            satellites.add(node1)\n",
    "            satellites.add(node2)\n",
    "    \n",
    "    return satellites\n",
    "\n",
    "def get_first_hop(G, source, target):\n",
    "    \"\"\"Get the first hop on the shortest path from source to target\"\"\"\n",
    "    try:\n",
    "        path = nx.shortest_path(G, source, target)\n",
    "        return path[1] if len(path) > 1 else None\n",
    "    except nx.NetworkXNoPath:\n",
    "        return None\n",
    "\n",
    "def process_single_node(args):\n",
    "    \"\"\"Process overhead calculation for a single node (for parallel processing)\"\"\"\n",
    "    node, current_topology, previous_topology, current_graph, previous_graph, \\\n",
    "    previous_first_hops, previous_aggregated_routes, current_satellites, previous_satellites, num_satellites = args\n",
    "    \n",
    "    stats = NodeStats()\n",
    "    \n",
    "    # Get node status\n",
    "    in_current = node in current_satellites\n",
    "    in_previous = node in previous_satellites\n",
    "    \n",
    "    if not in_current and not in_previous:\n",
    "        return node, stats, {}, {}\n",
    "        \n",
    "    # Calculate ISL changes\n",
    "    current_links = set((n1, n2) for (n1, n2), _ in current_topology \n",
    "                      if n1 == node or n2 == node)\n",
    "    previous_links = set((n1, n2) for (n1, n2), _ in previous_topology \n",
    "                       if n1 == node or n2 == node)\n",
    "    \n",
    "    deleted_links = previous_links - current_links\n",
    "    added_links = current_links - previous_links\n",
    "    \n",
    "    stats.deleted_isls = len(deleted_links)\n",
    "    stats.added_isls = len(added_links)\n",
    "    \n",
    "    # Handle route changes\n",
    "    current_first_hops = {}\n",
    "    current_aggregated_routes = defaultdict(set)  # next_hop -> {destinations}\n",
    "    \n",
    "    # If node changes from existing to non-existing\n",
    "    if in_previous and not in_current:\n",
    "        stats.deleted_routes = sum(1 for pair, hop in previous_first_hops.items() \n",
    "                                 if pair[0] == node)\n",
    "        stats.deleted_aggregated_routes = len(previous_aggregated_routes.get(node, {}))\n",
    "        return node, stats, {}, {}\n",
    "        \n",
    "    # If node changes from non-existing to existing or continues to exist\n",
    "    # Calculate routing information for current timestamp\n",
    "    for dst in current_satellites:\n",
    "        if dst != node:\n",
    "            current_hop = get_first_hop(current_graph, node, dst)\n",
    "            if current_hop is not None:\n",
    "                current_first_hops[(node, dst)] = current_hop\n",
    "                current_aggregated_routes[current_hop].add(dst)\n",
    "    \n",
    "    # If node is newly appeared\n",
    "    if not in_previous and in_current:\n",
    "        stats.added_routes = len(current_first_hops)\n",
    "        stats.added_aggregated_routes = len(current_aggregated_routes)\n",
    "        return node, stats, current_first_hops, current_aggregated_routes\n",
    "    \n",
    "    # Calculate regular route changes\n",
    "    for dst in current_satellites:\n",
    "        if dst == node:\n",
    "            continue\n",
    "            \n",
    "        pair = (node, dst)\n",
    "        prev_hop = previous_first_hops.get(pair)\n",
    "        curr_hop = current_first_hops.get(pair)\n",
    "        \n",
    "        if curr_hop and not prev_hop:  # New route\n",
    "            stats.added_routes += 1\n",
    "        elif not curr_hop and prev_hop:  # Deleted route\n",
    "            stats.deleted_routes += 1\n",
    "        elif curr_hop and prev_hop and curr_hop != prev_hop:  # Route change\n",
    "            stats.deleted_routes += 1  # Delete old route\n",
    "            stats.added_routes += 1    # Add new route\n",
    "            \n",
    "    # Calculate aggregated route changes\n",
    "    prev_agg_routes = previous_aggregated_routes.get(node, {})\n",
    "    \n",
    "    # Process all route entries\n",
    "    for prev_next_hop, prev_dsts in prev_agg_routes.items():\n",
    "        curr_dsts = current_aggregated_routes.get(prev_next_hop, set())\n",
    "        if not curr_dsts:\n",
    "            # This next hop is no longer used\n",
    "            stats.deleted_aggregated_routes += 1\n",
    "        else:\n",
    "            # Check if destination set needs updating\n",
    "            if prev_dsts != curr_dsts:\n",
    "                # Destination set has changed, needs updating\n",
    "                stats.deleted_aggregated_routes += 1\n",
    "                stats.added_aggregated_routes += 1\n",
    "    \n",
    "    # Process new next hops\n",
    "    for curr_next_hop, curr_dsts in current_aggregated_routes.items():\n",
    "        if curr_next_hop not in prev_agg_routes:\n",
    "            # Completely new next hop\n",
    "            stats.added_aggregated_routes += 1\n",
    "            \n",
    "    return node, stats, current_first_hops, current_aggregated_routes\n",
    "\n",
    "def process_single_node_tiny_leo(args):\n",
    "    \"\"\"Process TinyLEO overhead calculation for a single node (for parallel processing)\"\"\"\n",
    "    node, current_topology, previous_topology = args\n",
    "    \n",
    "    stats = NodeStats()\n",
    "    \n",
    "    # Get node status in current and previous timestamp\n",
    "    current_nodes = set()\n",
    "    for (n1, n2), _ in current_topology:\n",
    "        current_nodes.add(n1)\n",
    "        current_nodes.add(n2)\n",
    "        \n",
    "    previous_nodes = set()\n",
    "    for (n1, n2), _ in previous_topology:\n",
    "        previous_nodes.add(n1)\n",
    "        previous_nodes.add(n2)\n",
    "    \n",
    "    in_current = node in current_nodes\n",
    "    in_previous = node in previous_nodes\n",
    "    \n",
    "    # If node doesn't exist in either timestamp, return immediately\n",
    "    if not in_current and not in_previous:\n",
    "        return node, stats\n",
    "    \n",
    "    current_links = set((n1, n2) for (n1, n2), _ in current_topology \n",
    "                      if n1 == node or n2 == node)\n",
    "    previous_links = set((n1, n2) for (n1, n2), _ in previous_topology \n",
    "                       if n1 == node or n2 == node)\n",
    "    \n",
    "    # If node changes from existing to non-existing\n",
    "    if in_previous and not in_current:\n",
    "        stats.deleted_isls = len(previous_links)\n",
    "        return node, stats\n",
    "    \n",
    "    # If node changes from non-existing to existing\n",
    "    if not in_previous and in_current:\n",
    "        stats.added_isls = len(current_links)\n",
    "        return node, stats\n",
    "    \n",
    "    # Normal case: node exists in both timestamps\n",
    "    deleted_links = previous_links - current_links\n",
    "    added_links = current_links - previous_links\n",
    "    \n",
    "    stats.deleted_isls = len(deleted_links)\n",
    "    stats.added_isls = len(added_links)\n",
    "    \n",
    "    return node, stats\n",
    "\n",
    "def calculate_all_timestamps_overhead(all_inter_topology, all_intra_topology, num_satellites):\n",
    "    \"\"\"Calculate signaling overhead for all timestamps\"\"\"\n",
    "    ts_sdn_overhead = {}\n",
    "    tiny_leo_overhead = {}\n",
    "    \n",
    "    timestamps = sorted(all_inter_topology.keys())\n",
    "    num_processes = max(1, cpu_count() - 1)  # Reserve one core for the system\n",
    "    \n",
    "    # Process first timestamp\n",
    "    current_graph = build_topology_graph(all_inter_topology[timestamps[0]], \n",
    "                                       all_intra_topology[timestamps[0]])\n",
    "    current_satellites = get_topology_satellites(all_inter_topology[timestamps[0]], \n",
    "                                               all_intra_topology[timestamps[0]])\n",
    "    prev_first_hops = {}\n",
    "    prev_aggregated_routes = {}\n",
    "    \n",
    "    # Initialize first hop information\n",
    "    for src in current_satellites:\n",
    "        curr_aggregated_routes = defaultdict(set)\n",
    "        for dst in current_satellites:\n",
    "            if src != dst:\n",
    "                current_hop = get_first_hop(current_graph, src, dst)\n",
    "                if current_hop is not None:\n",
    "                    prev_first_hops[(src, dst)] = current_hop\n",
    "                    curr_aggregated_routes[current_hop].add(dst)\n",
    "        if curr_aggregated_routes:\n",
    "            prev_aggregated_routes[src] = curr_aggregated_routes\n",
    "    \n",
    "    print(f\"Processing {len(timestamps)} timestamps using {num_processes} processes...\")\n",
    "    \n",
    "    for t in tqdm(range(1, len(timestamps))):\n",
    "        current_topology = all_inter_topology[timestamps[t]]\n",
    "        previous_topology = all_inter_topology[timestamps[t-1]]\n",
    "        \n",
    "        # Build graphs\n",
    "        current_graph = build_topology_graph(current_topology, \n",
    "                                           all_intra_topology[timestamps[t]])\n",
    "        previous_graph = build_topology_graph(previous_topology, \n",
    "                                            all_intra_topology[timestamps[t-1]])\n",
    "        \n",
    "        current_satellites = get_topology_satellites(current_topology, \n",
    "                                                   all_intra_topology[timestamps[t]])\n",
    "        previous_satellites = get_topology_satellites(previous_topology, \n",
    "                                                    all_intra_topology[timestamps[t-1]])\n",
    "        \n",
    "        # Prepare parameters for parallel processing\n",
    "        ts_sdn_args = [(node, current_topology, previous_topology, current_graph, \n",
    "                       previous_graph, prev_first_hops, prev_aggregated_routes, \n",
    "                       current_satellites, previous_satellites, num_satellites) \n",
    "                      for node in range(num_satellites)]\n",
    "        \n",
    "        tiny_leo_args = [(node, current_topology, previous_topology) \n",
    "                        for node in range(num_satellites)]\n",
    "        \n",
    "        # Process all nodes in parallel\n",
    "        with Pool(processes=num_processes) as pool:\n",
    "            # Process TS-SDN\n",
    "            ts_results = pool.map(process_single_node, ts_sdn_args)\n",
    "            \n",
    "            # Process TinyLEO\n",
    "            tiny_results = pool.map(process_single_node_tiny_leo, tiny_leo_args)\n",
    "        \n",
    "        # Organize TS-SDN results\n",
    "        ts_messages = {}\n",
    "        ts_bytes = {}\n",
    "        ts_aggregated_bytes = {}\n",
    "        new_first_hops = {}\n",
    "        new_aggregated_routes = {}\n",
    "        \n",
    "        for node, stats, first_hops, aggregated_routes in ts_results:\n",
    "            if (stats.deleted_isls > 0 or stats.added_isls > 0 or \n",
    "                stats.deleted_routes > 0 or stats.added_routes > 0 or\n",
    "                stats.deleted_aggregated_routes > 0 or stats.added_aggregated_routes > 0):\n",
    "                ts_messages[node] = 1\n",
    "                ts_bytes[node] = stats.get_total_bytes()\n",
    "                ts_aggregated_bytes[node] = stats.get_total_aggregated_bytes()\n",
    "            if first_hops:\n",
    "                new_first_hops.update(first_hops)\n",
    "            if aggregated_routes:\n",
    "                new_aggregated_routes[node] = aggregated_routes\n",
    "        \n",
    "        # Organize TinyLEO results\n",
    "        tiny_messages = {}\n",
    "        tiny_bytes = {}\n",
    "        for node, stats in tiny_results:\n",
    "            if stats.deleted_isls > 0 or stats.added_isls > 0:\n",
    "                tiny_messages[node] = 1\n",
    "                tiny_bytes[node] = stats.get_total_bytes()\n",
    "        \n",
    "        # Store results\n",
    "        ts_sdn_overhead[timestamps[t]] = {\n",
    "            'messages': ts_messages,\n",
    "            'bytes': ts_bytes,\n",
    "            'aggregated_bytes': ts_aggregated_bytes,\n",
    "            'detailed_stats': {\n",
    "                node: {\n",
    "                    'deleted_isls': stats.deleted_isls,\n",
    "                    'added_isls': stats.added_isls,\n",
    "                    'deleted_routes': stats.deleted_routes,\n",
    "                    'added_routes': stats.added_routes,\n",
    "                    'deleted_aggregated_routes': stats.deleted_aggregated_routes,\n",
    "                    'added_aggregated_routes': stats.added_aggregated_routes\n",
    "                }\n",
    "                for node, stats, _, _ in ts_results if (\n",
    "                    stats.deleted_isls > 0 or stats.added_isls > 0 or\n",
    "                    stats.deleted_routes > 0 or stats.added_routes > 0 or\n",
    "                    stats.deleted_aggregated_routes > 0 or stats.added_aggregated_routes > 0\n",
    "                )\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        tiny_leo_overhead[timestamps[t]] = {\n",
    "            'messages': tiny_messages,\n",
    "            'bytes': tiny_bytes\n",
    "        }\n",
    "        \n",
    "        # Update previous information\n",
    "        prev_first_hops = new_first_hops\n",
    "        prev_aggregated_routes = new_aggregated_routes\n",
    "    \n",
    "    return ts_sdn_overhead, tiny_leo_overhead\n",
    "\n",
    "def save_and_analyze_results(ts_sdn_overhead, tiny_leo_overhead, output_dir):\n",
    "    \"\"\"Save and analyze results\"\"\"\n",
    "    # Save raw data\n",
    "    np.save(os.path.join(output_dir, \"ts_sdn_overhead_573_11_11_distancedt_global_max_new.npy\"), \n",
    "            ts_sdn_overhead)\n",
    "    np.save(os.path.join(output_dir, \"tiny_leo_overhead_573_11_11_distancedt_global_max_new.npy\"), \n",
    "            tiny_leo_overhead)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    ts_total_messages = sum(sum(t['messages'].values()) \n",
    "                          for t in ts_sdn_overhead.values())\n",
    "    ts_total_bytes = sum(sum(t['bytes'].values()) \n",
    "                        for t in ts_sdn_overhead.values())\n",
    "    ts_total_aggregated_bytes = sum(sum(t['aggregated_bytes'].values()) \n",
    "                                  for t in ts_sdn_overhead.values())\n",
    "    \n",
    "    tiny_total_messages = sum(sum(t['messages'].values()) \n",
    "                            for t in tiny_leo_overhead.values())\n",
    "    tiny_total_bytes = sum(sum(t['bytes'].values()) \n",
    "                          for t in tiny_leo_overhead.values())\n",
    "    \n",
    "    # Count detailed operation quantities\n",
    "    total_stats = defaultdict(int)\n",
    "    for timestamp_data in ts_sdn_overhead.values():\n",
    "        for node_stats in timestamp_data['detailed_stats'].values():\n",
    "            for key, value in node_stats.items():\n",
    "                total_stats[key] += value\n",
    "    \n",
    "    # Save statistics\n",
    "    with open(os.path.join(output_dir, \"overhead_statistics_573_11_11_distancedt_global_max.txt\"), 'w') as f:\n",
    "        f.write(\"Signaling Overhead Statistics\\n\")\n",
    "        f.write(\"===========================\\n\\n\")\n",
    "        \n",
    "        f.write(\"TS-SDN:\\n\")\n",
    "        f.write(f\"Total messages: {ts_total_messages}\\n\")\n",
    "        f.write(f\"Total bytes (non-aggregated): {ts_total_bytes}\\n\")\n",
    "        f.write(f\"Total bytes (aggregated): {ts_total_aggregated_bytes}\\n\")\n",
    "        f.write(f\"Average messages per timestamp: {ts_total_messages/len(ts_sdn_overhead):.2f}\\n\")\n",
    "        f.write(f\"Average bytes per timestamp (non-aggregated): {ts_total_bytes/len(ts_sdn_overhead):.2f}\\n\")\n",
    "        f.write(f\"Average bytes per timestamp (aggregated): {ts_total_aggregated_bytes/len(ts_sdn_overhead):.2f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"Detailed Operations:\\n\")\n",
    "        for key, value in total_stats.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"TinyLEO:\\n\")\n",
    "        f.write(f\"Total messages: {tiny_total_messages}\\n\")\n",
    "        f.write(f\"Total bytes: {tiny_total_bytes}\\n\")\n",
    "        f.write(f\"Average messages per timestamp: {tiny_total_messages/len(tiny_leo_overhead):.2f}\\n\")\n",
    "        f.write(f\"Average bytes per timestamp: {tiny_total_bytes/len(tiny_leo_overhead):.2f}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration parameters\n",
    "    input_dir = \"data\"\n",
    "    output_dir = \"data\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load topology data\n",
    "    print(\"Loading topology data...\")\n",
    "    all_inter_topology = np.load(os.path.join(input_dir, \n",
    "        \"all_inter_topology_573_11_11_distancedt_global_max.npy\"), \n",
    "        allow_pickle=True).item()\n",
    "    all_intra_topology = np.load(os.path.join(input_dir, \n",
    "        \"all_intra_topology_573_11_11_distancedt_global_max.npy\"), \n",
    "        allow_pickle=True).item()\n",
    "    \n",
    "    num_satellites = 1762\n",
    "    \n",
    "    # Calculate signaling overhead\n",
    "    start_time = time.time()\n",
    "    ts_sdn_overhead, tiny_leo_overhead = calculate_all_timestamps_overhead(\n",
    "        all_inter_topology, all_intra_topology, num_satellites\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Processing time: {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Save and analyze results\n",
    "    save_and_analyze_results(ts_sdn_overhead, tiny_leo_overhead, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
